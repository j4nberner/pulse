name: "GRUModel"
pretrained_model_path: null # Path to pretrained model (if any)
params:
  # Basic configuration
  trainer_name: "GRUTrainer"
  type: "convDL" # Type of model (convML for conventional machine learning, convDL for conventional deep learning and LLM for large language models)
  save_checkpoint_freq: 5
  verbose: 2 # Controls verbosity of output (0 = silent, 1 = log every 10 batches, 2 = log every batch)
  # Training parameters
  num_epochs: 60
  earlystopping_patience: 5
  # Model architecture
  hidden_dim: 64
  layer_dim: 2
  dropout_rate: 0.3
  fc_layers: [64, 16]
  activation: "leaky_relu" # Activation function (relu, leaky_relu, gelu)
  # Optimizer settings
  optimizer_name: "adam" # Options: adam, adamw, sgd
  learning_rate: 0.00001
  weight_decay: 1e-6
  grad_clip_max_norm: 1.0 # max_norm = 1.0 (LSTM, GRU) or 2.0 (CNN, InceptionTime)
  # Scheduler settings
  scheduler_factor: 0.5
  scheduler_patience: 3
  min_lr: 1e-6



  # # YAIB implementation (-> middle values from the ranges are implemented above):
  # optimizer_name: "adam"
  # weight_decay: 1e-6
  # lr: (1e-6, 1e-4, "log")
  # hidden_dim: (32, 512, "log") -> used 32, 128 -> middle value: 64
  # layer_dim: (1, 10) -> used 1, 3 -> middle value: 2
