name: "XGBoost"
pretrained_model_path: null # Path to pretrained model (if any)
params:
  # Basic configuration
  trainer_name: "XGBoostTrainer" # Class that implements the training process
  type: "ML" # Type of model (ML for machine learning, DL for deep learning)
  tune_hyperparameters: true # Whether to tune hyperparameters
  objective: "binary:logistic" # For binary classification tasks like AKI prediction
  n_estimators: 1000 # Number of boosting rounds/trees to build
  learning_rate: 0.1 # Controls how much each tree contributes to the final model (eta)
  random_state: 42 # Seed for reproducible results
  verbosity: 0 # Controls the level of logging output (0: silent, 1: warnings, 2: info)
  # Tree structure parameters
  max_depth: 6 # Maximum depth of each tree (prevents overfitting)
  gamma: 0 # Minimum loss reduction required for a split
  min_child_weight: 1 # Minimum sum of instance weight needed in a child node
  # Sampling parameters
  subsample: 0.8 # Fraction of samples used for tree building (prevents overfitting)
  colsample_bytree: 0.8 # Fraction of features used for tree building
  # Regularization parameters
  reg_alpha: 0 # L1 regularization on weights (0 = no regularization)
  reg_lambda: 1 # L2 regularization on weights (higher = more regularization)
  # Performance and evaluation parameters
  n_jobs: 1 # Number of parallel threads (-1 to use all CPUs)
  tree_method: "hist" # Algorithm to build trees (hist = faster histogram-based approach)
  eval_metric: "auc" # Metric used for validation data evaluation
  early_stopping_rounds: 20 # Stop training if performance doesn't improve for this many rounds
