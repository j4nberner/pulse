name: "XGBoost"
params:
  # Basic configuration
  trainer_name: "XGBoostTrainer" # Class that implements the training process
  objective: "binary:logistic" # For binary classification tasks like AKI prediction
  n_estimators: 100 # Number of boosting rounds/trees to build
  learning_rate: 0.1 # Controls how much each tree contributes to the final model (eta)
  random_state: 42 # Seed for reproducible results
  verbosity: 1 # Controls the level of logging output (0: silent, 1: warnings, 2: info)
  # Tree structure parameters
  max_depth: 3 # Maximum depth of each tree (prevents overfitting)
  gamma: 0 # Minimum loss reduction required for a split
  min_child_weight: 1 # Minimum sum of instance weight needed in a child node
  # Sampling parameters
  subsample: 0.8 # Fraction of samples used for tree building (prevents overfitting)
  colsample_bytree: 0.8 # Fraction of features used for tree building
  # Regularization parameters
  reg_alpha: 0 # L1 regularization on weights (0 = no regularization)
  reg_lambda: 1 # L2 regularization on weights (higher = more regularization)
  scale_pos_weight: 1 # Balance positive and negative weights (for imbalanced datasets)
  # Performance and evaluation parameters
  n_jobs: 1 # Number of parallel threads (-1 to use all CPUs)
  tree_method: "hist" # Algorithm to build trees (hist = faster histogram-based approach)
  eval_metric: "auc" # Metric used for validation data evaluation
  early_stopping_rounds: 10 # Stop training if performance doesn't improve for this many rounds
