# Configuration file for pulse benchmark

# General settings
general:
  app_name: "Pulse"
  version: "1.0.0"
  debug_mode: true
  use_scratch: true

# Weight and Biases (wandb) logging configuration
wandb:
  enabled: false
  entity: "berner"
  # experiment_name: "experiment_1"

###### Application specific settings ######
base_path: "/Users/sophiaehlers/Documents/pulse" #"/path/to/pulse" "/cluster/project/bmds_lab/sepsis_sophia/pulse" 

# Tasks to process
tasks:
  # - "mortality"
  - "aki"
  # - "sepsis"
  # - "harmonized_icu"

# Datasets to process
datasets:
  - "hirid"
  # - "miiv"
  # - "eicu"

# _______ Preprocessing Baseline _______

# Random seed for reproducibility
random_seed: 42

# Options for preprocessing
preprocessing:
  replace_outliers: true          # Whether to replace outliers with NaNs
  flag_na: true                   # Whether to flag NA values with binary indicators
  standardize: true               # Whether to standardize features
  save_data: false                # Whether to save preprocessed data
  split_ratios:                   # Train/Val/Test split ratios
    train: 0.7
    val: 0.1
    test: 0.2

# TODO: add config -> use presaved scripts, print set statistics, ...
# TODO: change folder name of splits to automatic naming based on preprocessing configuration
# TODO: add logging for the preprocessing_baseline setup
# TODO: Change the config preprocessing to preprocessing_baseline (to be consistent with preprocessing_advanced)

# _______ Preprocessing Advanced _______
preprocessing_advanced:
  windowing:
    enabled: true                    # Enable/disable windowing
    data_window: 6                   # Size of the data window
    prediction_window: 0             # Size of the prediction window (0 = predict at the end of window)
    step_size: 1                     # Step size for sliding windows
    save_data: false                 # Whether to save windowed data

# _______ Model Training _______

benchmark_settings:
  batch_size: 100
  num_epochs: 10
  learning_rate: 0.001
  optimizer: "adam" # Options: adam, sgd, rmsprop

models:
  - name: "RandomForest"
    params:
      trainer_name: "RandomForestTrainer"
      n_estimators: 200        # Number of trees in the forest
      n_jobs: 24               # Number of CPU cores to use (-1 for all available, None for 1 CPU)
      max_depth: 20            # Maximum depth of the tree (null for unlimited)
      min_samples_split: 2     # Minimum samples required to split internal node
      min_samples_leaf: 2      # Minimum samples required at a leaf node
      max_features: "sqrt"     # Number of features to consider (options: "sqrt", "log2", int, float)
      bootstrap: true          # Whether to use bootstrap samples
      oob_score: false         # Whether to use out-of-bag samples
      random_state: 42       # Random seed for reproducibility (scikit default = None)
      verbose: 0               # Controls verbosity of output

  # - name: "XGBoost"
  #   params:
  #     trainer_name: "XGBoostTrainer"
  #     n_estimators: 100
  #     learning_rate: 0.1
  #     max_depth: 3
  #     subsample: 0.8
  #     colsample_bytree: 0.8
  #     objective: "binary:logistic"
  # - name: "SimpleDLModel"
  #   params:
  #     trainer_name: "SimpleDLTrainer"
  #     input_size: 3
  #     hidden_size: 128
  #     output_size: 10
